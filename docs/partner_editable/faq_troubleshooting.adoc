// Add any tips or answers to anticipated questions.

== FAQ

*Q.* I encountered a *CREATE_FAILED* error when I launched the Quick Start.

*A.* If AWS CloudFormation fails to create the stack, relaunch the template with *Rollback on failure* set to *Disabled*. This setting is under *Advanced* in the AWS CloudFormation console on the *Configure stack options* page. With this setting, the stack’s state is retained, and the instance keeps running so that you can troubleshoot the issue. (For Windows, look at the log files in `%ProgramFiles%\Amazon\EC2ConfigService` and `C:\cfn\log`.)
// Customize this answer if needed. For example, if you’re deploying on Linux instances, either provide the location for log files on Linux or omit the final sentence. If the Quick Start has no EC2 instances, revise accordingly (something like "and the assets keep running").

WARNING: When you set *Rollback on failure* to *Disabled*, you continue to incur AWS charges for this stack. Delete the stack when you finish troubleshooting.

For more information, see https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html[Troubleshooting AWS CloudFormation^].

*Q.* I encountered a size-limitation error when I deployed the AWS CloudFormation templates.

*A.* Launch the Quick Start templates from the links in this guide or from another S3 bucket. If you deploy the templates from a local copy on your computer or from a location other than an S3 bucket, you might encounter template-size limitations. For more information, see http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cloudformation-limits.html[AWS CloudFormation quotas^].

== Troubleshooting

*Q.* I want to attach a console session.

*A.* Use kubectl to attach a particular CNFW's console session. From there you can run any command that would not result in a change in the running configuration of CNFW. For example: capture, packet-tracer, show run, etc.

`kubectl attach -it <sfcn-enforcer-pod-name> -n $NS -c cnfw`

Where `$NS` represents the namespace of the pod.

You might be required to run the enable command before being able to execute any of the show commands. The enable command would require you to enter a password when run for the first time. This password is not persistent and would be reset once the sfcn-enforcer pod restarts or unscheduled.

*Q.* I want to SSH to EKS nodes.

*A.* Install ec2instanceconnect (pip install ec2instanceconnectcli), and add the following to your shell profile:

[source,*EKS mssh*,options="nowrap"]

EKS mssh
function sshn() {
  PS3="Select a node to ssh: "
  select node in $(kubectl get nodes | grep -v NAME | awk '{print $1}')
  do
    echo "Selected:  $node"
    INSTANCE_ID=$(kubectl describe node $node | awk -F'/' '/ProviderID/{print $5}')
    REGION=$(kubectl describe nodes $node | awk -F'=' '/topology.kubernetes.io\/region/{print $2}')
    mssh --region $REGION ubuntu@$INSTANCE_ID
    break
    done
    return $?
}

Execute sshn, and select the node you wish to SSH to when prompted.

*Q.* I want to check the deployment status.

*A.* After creating an ASAConfiguration object, you can track the status of its deployment using the status field. See the ASAConfiguration CRD for detailed information on the status field.

The ASAConfiguration status field currently has 2 status conditions:

*Valid:* This condition provides information if the CLI lines present in the ASAConfiguration object are valid or not. There are certain CLI commands that are blocked from using to maintain system integrity. If ASAConfiguration contains any of those commands, the Valid condition would be false. Valid condition's message field would provide detailed information on which command is blocked.

*Deployed:* This condition provides information if the CLI lines have been deployed to all applicable nodes in the cluster. This condition becomes true only when the CLI lines are deployed successfully across all applicable nodes. This status condition shows a summary on how many nodes to which the cluster has been deployed.

In the event the deployed status shows the configuration has been deployed to only x nodes of y nodes, check the NodeConfiguration's status field of the nodes present in the cluster.

`kubectl describe nodeconfiguration <node_name> -n $NS`

*Q.* I want to view container logs.

*A.* By default, the system forwards the combined logs for all SFCN-related containers running in the cluster to a central logging service. Use the following kubectl command to view the logs:

```[source,$ POD=$(grep -m 1 logging-server,options="nowrap"]
POD=$(grep -m 1 logging-server <<< "`kubectl get pods -n $NS --no-headers=true -o custom-columns=':metadata.name'`")
kubectl logs -n $NS pod/$POD -c logging-server```

Optionally, the parameter -f can be added to continuously tail the log. Note that $NS represents the namespace of the pod.

Alternatively, it is possible to login to the logging server:

`kubectl exec -n $NS -it $POD -c logging-server -- /bin/sh`

```[source,$ cd /var/log/logs,options="nowrap"]
$ cd /var/log/logs
$ ls
  messages-kv.log  messages.log
$ grep asac messages-kv.log
  Mar 30 21:30:27 ip-192-168-118-112 [2021/03/30 21:30:27] [ info] inotify_fs_add(): inode=1536390 watch_fd=62 name=/var/log/containers/sfcn-cli-validation-<truncated>.log
  Mar 30 21:33:10 ip-192-168-118-112 [2021/03/30 21:33:10] [ info] inotify_fs_add(): inode=1536539 watch_fd=64 name=/var/log/containers/sfcn-cli-validation-<truncated>.log```

Logs stored on the logging server are routinely rotated and cleaned up. If you need to preserve logs, save the files before the house-cleaning jobs run.

Unified container logs are formatted as follows:

```<year-month-day> <time> <node name> <pod namespace> <pod name> <container name> -- <log message> -- <extra JSON-formatted log metadata>

For example, an entry from a configuration pod may look like this:
[source,2021-03-18 21:28:00 ,options="nowrap"]
2021-03-18 21:28:00 ip-1-1-1-1.ec2.internal sfcn-system sfcn-configurer-1111111111-22222 manager --
2021-03-18T21:28:00.806Z INFO controllers.SNMPExporter Reconciliation started {"namespace": "sfcn-system"} -- {"_SDATA":{"k8s":{
The unified logs are also stored and periodically rotated on the node where the logging-server is running at /home/ubuntu/var-log/.```

Standard output and standard error logs for individual containers can also be viewed using the kubectl log command:

`kubectl logs pod/<pod_name> -c <container_name>`
