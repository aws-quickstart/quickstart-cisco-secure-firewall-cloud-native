AWSTemplateFormatVersion: "2010-09-09"
Description: Nested stack of Cisco Secure Firewall Cloud Native Quickstart entrypoints that deployes all CSFCN resources.
Parameters:
  QSS3BucketName:
    AllowedPattern: ^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$
    ConstraintDescription: Quick Start bucket name can include numbers, lowercase
      letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen
      (-).
    Default: aws-quickstart
    Description: S3 bucket name for the Quick Start assets. This string can include
      numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start
      or end with a hyphen (-).
    Type: String
  QSS3KeyPrefix:
    AllowedPattern: ^[0-9a-zA-Z-/.]*$
    ConstraintDescription: Quick Start key prefix can include numbers, lowercase letters,
      uppercase letters, hyphens (-), periods (.) and forward slashes (/).
    Default: quickstart-cisco-secure-firewall-cloud-native/
    Description: The S3 key prefix for the Quick Start assets. Quick Start key prefix
      can include numbers, lowercase letters, uppercase letters, hyphens (-), periods (.) and
      forward slashes (/).
    Type: String
  QSS3BucketRegion:
    Default: 'us-east-1'
    Description: AWS Region where the Quick Start S3 bucket (QSS3BucketName) is hosted. If you use your own bucket, you must specify this value.
    Type: String

  VPCID:
    Type: "AWS::EC2::VPC::Id"
    Description: ID of your existing VPC (e.g., vpc-0343606e).
  PublicSubnet1ID:
    Type: "AWS::EC2::Subnet::Id"
    Description: ID of the public DMZ subnet 1 (CNFW default 1), located in Availability Zone 1.
  PrivateSubnet1ID:
    Type: "AWS::EC2::Subnet::Id"
    Description: ID of the private subnet 1, located in Availability Zone 1. Must have "kubernetes.io/role/internal-elb" tag attached.
  PublicSubnet2ID:
    Type: "AWS::EC2::Subnet::Id"
    Description: ID of the public DMZ subnet 2 (CNFW default 2), located in Availability Zone 2.
  PrivateSubnet2ID:
    Type: "AWS::EC2::Subnet::Id"
    Description: ID of the private subnet 2, located in Availability Zone 2. Must have "kubernetes.io/role/internal-elb" tag attached.
  PublicSubnetRouteTableID:
    Description: ID of VPC public subnet route table (e.g., rtb-0b9ce78da592f11e0).
    Type: String
  CNFWOutsideSubnet1CIDR:
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/(1[6-9]|2[0-8]))$
    ConstraintDescription: CIDR block parameter must be in the form x.x.x.x/16–28.
    Description: CIDR block for the CNFW outside subnet 1, located in Availability Zone 1.
    Type: String
  CNFWInsideSubnet1CIDR:
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/(1[6-9]|2[0-8]))$
    ConstraintDescription: CIDR block parameter must be in the form x.x.x.x/16–28.
    Description: CIDR block for the CNFW inside subnet 1, located in Availability Zone 1.
    Type: String
  CNFWOutsideSubnet2CIDR:
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/(1[6-9]|2[0-8]))$
    ConstraintDescription: CIDR block parameter must be in the form x.x.x.x/16–28.
    Description: CIDR block for the CNFW outside subnet 2, located in Availability Zone 2.
    Type: String
  CNFWInsideSubnet2CIDR:
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/(1[6-9]|2[0-8]))$
    ConstraintDescription: CIDR block parameter must be in the form x.x.x.x/16–28.
    Description: CIDR block for the CNFW inside subnet 2, located in Availability Zone 2.
    Type: String

  EKSClusterName:
    Type: String
    MaxLength: 26 # Issue https://github.com/aws-quickstart/quickstart-amazon-eks/issues/314

  KubernetesVersion:
    Type: String
    Default: 1.18
    AllowedValues:
      - 1.18
    Description: The desired Kubernetes version for your cluster.

  KeyPairName:
    Description: Name of an existing key pair, which allows you to securely connect to your instance after it launches.
    Type: AWS::EC2::KeyPair::KeyName

  EKSAdminUserArn:
    Type: String
    Description: Resource name of the IAM user added to the aws-auth ConfigMap of the EKS cluster.
    # Make the param required so that the admin IAM user is assigned
    AllowedPattern: ".+"

  AdditionalEKSAdminRoleArn:
    Type: String
    Default: ""
    Description: Resource name of additional IAM role added to the aws-auth ConfigMap of the EKS cluster.

  EKSClusterLoggingTypes:
    Type: String
    Default: ""
    AllowedPattern: ^$|^\s*(api|audit|authenticator|controllerManager|scheduler)\s*(,\s*(api|audit|authenticator|controllerManager|scheduler)\s*)*$
    ConstraintDescription: Comma separated list of allowed values - api, audit, authenticator, controllerManager and scheduler.
    Description: |
      Cluster control plane logging options (api, audit, authenticator, controllerManager and scheduler); comma separated.

  ControllerConfigurationTier:
    Description: Instance tier to be used by controller workers.
    Default: "vCPU4"
    Type: String
    AllowedValues: [ "vCPU4" ]
    ConstraintDescription: must specify vCPU4

  EnforcerConfigurationTier:
    Description: Instance tier to be used by enforcer workers.
    Default: "vCPU4"
    Type: String
    AllowedValues: [ "vCPU4" ]
    ConstraintDescription: must specify vCPU4

  ControlNodeGroupMaxSize:
    Type: Number
    Default: 2
    Description: The maximum number of control plane worker nodes.

  ControlNodeGroupDesiredSize:
    Type: Number
    Default: 1
    Description: The desired number of control plane worker nodes.

  StorageType:
    Type: String
    AllowedValues: [ local, efs ]
    Default: local
    Description: The type of a storage used to keep logs and deployments files.

  EnforcerNodeGroupMaxSize:
    Type: Number
    Default: 5
    Description: The maximum number of data plane worker nodes.

  EnforcerNodeGroupDesiredSize:
    Type: Number
    Default: 1
    Description: The desired number of data plane worker nodes.

  EIPAttachmentMode:
    Type: String
    Default: "outside"
    AllowedValues:
      - none
      - outside
      - inside
      - both
    Description: CNFW interfaces to attach Elastic IPs.

  EnforcerCacheType:
    Type: String
    Default: "none"
    AllowedValues:
      - none
      - elasticache
    Description: Type of External Cache to use for CNFW.

  EnforcerCacheNodeType:
    Type: String
    Default: "cache.m5.large"
    AllowedValues: [ cache.m5.xlarge, cache.m5.large ]
    Description: EC2 instance type to use as Cache node, check region availaibility before selection.

  EnforcerCacheAuthToken:
    Type: String
    NoEcho: true
    MaxLength: 128
    Default: ""
    AllowedPattern: "([a-zA-Z0-9!&#$^><-]{16,128})?"
    ConstraintDescription: Must be blank or contain at least 16 and up to 128 alphanumeric or allowed special characters.
    Description: |
      Blank or 16-128 alphanumeric characters. Only permitted special characters are !, &, #, $, ^, <, >, and -.
      Takes effect only if in-transit encryption is enabled.

  EnforcerCacheStorageKey:
    Type: String
    NoEcho: true
    MaxLength: 64
    Default: ""
    AllowedPattern: "^([A-F0-9]{64})?$"
    ConstraintDescription: Must be a string of length 64 consisting of uppercase hexadecimal characters (A-F,0-9).
    Description: |
      User provided key to encrypt sensitive data in the cache.
      Must consist of 64 uppercase hexadecimal characters (A-F,0-9).
      Required when 'elasticache' cache type is specified.

  EnforcerCacheTransitEncryption:
    Type: String
    AllowedValues: [ Enabled, Disabled ]
    Default: Enabled
    Description: Cannot be changed after creation, update requires replacement.

  EnforcerCachePort:
    Type: Number
    Default: 6379
    MinValue: 1
    MaxValue: 65535
    Description: Port number to use for communicating with Redis server.

  SecureFirewallDataplane:
    Type: String
    AllowedValues: [ Enabled, Disabled ]
    Default: Enabled
    Description: This is enabled by default for a full deployment with control plane and data plane components. To deploy control plane components only (for multi-tenancy), disable this setting.

  FirewallVersion:
    Type: String
    Default: v1.0.0
    AllowedPattern: ".+"
    Description: Helm chart version of the product.

  SystemNamespace:
    Type: String
    Default: sfcn-system
    AllowedPattern: ".+"
    Description: The namespace where product components are installed.

  SmartLicenseToken:
    Type: String

  MaxLicenseCount:
    Type: Number

  EnforcerServiceRoles:
    Type: String
    Default: default
    AllowedPattern: "[a-zA-Z0-9-]{3,15}(,[a-zA-Z0-9-]{3,15})*"
    AllowedValues: [ "default", "default,vpnredirector" ]
    ConstraintDescription: |
      Role should be alphanumeric and can contain the special character, hyphen (-). Role must be at least 3 characters
      and upto 15 characters long.
    Description: |
      The service role of the data plane node. Leave the setting as "default" if you selected 1 data plane node. Each additional service role requires a corresponding data plane node.

  EnforcerAutoscaling:
    Type: String
    AllowedValues: [ Enabled, Disabled ]
    Default: Disabled
    Description: Enable the default Enforcer autoscaling.

Mappings:
  WorkerAMIMap:
    us-east-1:
      "118": ami-04e0aace2faaea0c0
      "119": ami-06f995c5e133ad46c
    us-east-2:
      "118": ami-0883f55ce0673c383
      "119": ami-00238099ab28c3007
  ConfigurationTierMap:
    "vCPU4":
      instanceType: m5.xlarge
      hugePagesSize: "128"

Rules:
  EnforcerElasticacheRule:
    RuleCondition: !Equals [ !Ref EnforcerCacheType, elasticache ]
    Assertions:
      - Assert: !Equals [ !Ref SecureFirewallDataplane, Enabled ]
        AssertDescription: Cache type can be set to "elasticache" only if firewall data plane is "Enabled".
      - Assert: !Not [ !Equals [ !Ref EnforcerCacheStorageKey, "" ] ]
        AssertDescription: Cache storage key must not be empty when cache type is set to "elasticache".

Conditions:
  UsingDefaultBucket: !Equals [ !Ref QSS3BucketName, 'aws-quickstart' ]
  EnableEFS: !Equals [ !Ref StorageType, "efs" ]
  EnforcerCacheTypeIsElastiCache: !Equals [ !Ref EnforcerCacheType, elasticache ]

Resources:

  CloudFormationKubernetesVPCRoleExists:
    Type: Custom::CliQuery
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-ResourceReader'
      AwsCliCommand: "iam list-roles --query 'Roles[?RoleName==`CloudFormation-Kubernetes-VPC`].RoleName | {RoleName: [0]}'"
      IdField: 'RoleName'

  IamStack:
    Type: "AWS::CloudFormation::Stack"
    Properties:
      TemplateURL: !Sub
        - 'https://${S3Bucket}.s3.${S3Region}.${AWS::URLSuffix}/${QSS3KeyPrefix}submodules/quickstart-amazon-eks/templates/amazon-eks-iam.template.yaml'
        - S3Region: !If [ UsingDefaultBucket, !Ref 'AWS::Region', !Ref QSS3BucketRegion ]
          S3Bucket: !If [ UsingDefaultBucket, !Sub '${QSS3BucketName}-${AWS::Region}', !Ref QSS3BucketName ]
      Parameters:
        QSS3BucketName: !Ref QSS3BucketName
        CreateBastionRole: "Disabled"
        BastionIAMRoleName: ""
        CloudFormationKubernetesVPCRoleExists: !Ref CloudFormationKubernetesVPCRoleExists

  VPCCIDR:
    Type: Custom::CliQuery
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-ResourceReader'
      AwsCliCommand: !Sub "ec2 describe-vpcs --vpc-id ${VPCID} --query 'Vpcs[0].{CidrBlock: CidrBlock}'"
      IdField: 'CidrBlock'

  # Inside/Outside subnets are created for the first two Availability Zones of the current region.
  # Each subnet uses a /20 netmask (4096 IPv4 addresses).
  # Creating subnets for all AZs dynamically is not supported by vanilla CloudFormation templates.

  OutsideSubnetAZ1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPCID
      CidrBlock: !Ref CNFWOutsideSubnet1CIDR
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [ 0, !GetAZs "" ]
      Tags:
        - Key: Name
          Value: !Sub [ "${pref}_outside_${az}", { pref: !Ref EKSClusterName, az: !Select [ 0, !GetAZs "" ] } ]
        - Key: !Sub "kubernetes.io/cluster/${EKSClusterName}"
          Value: "shared"
        - Key: kubernetes.io/role/elb
          Value: "1"

  OutsideSubnetAZ1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref OutsideSubnetAZ1
      RouteTableId: !Ref PublicSubnetRouteTableID

  InsideSubnetAZ1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPCID
      CidrBlock: !Ref CNFWInsideSubnet1CIDR
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [ 0, !GetAZs "" ]
      Tags:
        - Key: Name
          Value: !Sub [ "${pref}_inside_${az}", { pref: !Ref EKSClusterName, az: !Select [ 0, !GetAZs "" ] } ]
        - Key: !Sub "kubernetes.io/cluster/${EKSClusterName}"
          Value: "shared"
        - Key: kubernetes.io/role/elb
          Value: "1"

  InsideSubnetAZ1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref InsideSubnetAZ1
      RouteTableId: !Ref PublicSubnetRouteTableID

  OutsideSubnetAZ2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPCID
      CidrBlock: !Ref CNFWOutsideSubnet2CIDR
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [ 1, !GetAZs "" ]
      Tags:
        - Key: Name
          Value: !Sub [ "${pref}_outside_${az}", { pref: !Ref EKSClusterName, az: !Select [ 1, !GetAZs "" ] } ]
        - Key: !Sub "kubernetes.io/cluster/${EKSClusterName}"
          Value: "shared"
        - Key: kubernetes.io/role/elb
          Value: "1"

  OutsideSubnetAZ2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref OutsideSubnetAZ2
      RouteTableId: !Ref PublicSubnetRouteTableID

  InsideSubnetAZ2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPCID
      CidrBlock: !Ref CNFWInsideSubnet2CIDR
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [ 1, !GetAZs "" ]
      Tags:
        - Key: Name
          Value: !Sub [ "${pref}_inside_${az}", { pref: !Ref EKSClusterName, az: !Select [ 1, !GetAZs "" ] } ]
        - Key: !Sub "kubernetes.io/cluster/${EKSClusterName}"
          Value: "shared"
        - Key: kubernetes.io/role/elb
          Value: "1"

  InsideSubnetAZ2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref InsideSubnetAZ2
      RouteTableId: !Ref PublicSubnetRouteTableID

  # Security Groups

  DefaultSecurityGroupId:
    Type: Custom::CliQuery
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-ResourceReader'
      AwsCliCommand: !Sub 'ec2 describe-security-groups --filters "Name=group-name,Values=default" "Name=vpc-id,Values=${VPCID}" --query "SecurityGroups[0]"'
      IdField: 'GroupId'

  ControlPlaneSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: EKS cluster control plane security group.
      VpcId: !Ref VPCID
      Tags:
        - Key: Name
          Value: !Sub "${EKSClusterName}-control-plane"
        - Key: !Sub "kubernetes.io/cluster/${EKSClusterName}"
          Value: "owned"
      SecurityGroupIngress:
        - Description: "Allow pods to communicate with the EKS cluster API."
          IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: 0.0.0.0/0
      SecurityGroupEgress:
        - Description: "Allow cluster egress access to the Internet."
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 0
          CidrIp: 0.0.0.0/0

  ControlPlaneSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow SG members to access k8s api.
      GroupId: !Ref ControlPlaneSecurityGroup
      SourceSecurityGroupId: !Ref ControlPlaneSecurityGroup
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ControlPlaneSecurityGroupIngressForWorkers:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: "Allow pods running on workers to send communication to cluster primary security group."
      GroupId: !Ref ControlPlaneSecurityGroup
      SourceSecurityGroupId: !Ref WorkersSecurityGroup
      IpProtocol: "-1"
      FromPort: 0
      ToPort: 65535

  WorkersSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: EKS cluster worker nodes security group.
      VpcId: !Ref VPCID
      Tags:
        - Key: Name
          Value: !Sub "${EKSClusterName}-workers"
        - Key: !Sub "kubernetes.io/cluster/${EKSClusterName}"
          Value: "owned"
      SecurityGroupIngress:
        - Description: "Allow SSH access."
          IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0
        - Description: "Allow workers pods to receive communication from the cluster control plane."
          SourceSecurityGroupId: !Ref ControlPlaneSecurityGroup
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 0
      SecurityGroupEgress:
        - Description: "Allow all egress."
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 0
          CidrIp: 0.0.0.0/0

  WorkersSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: "Allow inter-worker communication."
      GroupId: !Ref WorkersSecurityGroup
      SourceSecurityGroupId: !Ref WorkersSecurityGroup
      IpProtocol: "-1"
      FromPort: 0
      ToPort: 0

  WorkersSecurityGroup0:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for default interface of cluster workers.
      VpcId: !Ref VPCID
      Tags:
        - Key: Name
          Value: !Sub "${EKSClusterName}-workers-0"
      SecurityGroupIngress:
        - Description: "Allow connections from first CNFW interface."
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 65535
          SourceSecurityGroupId: !Ref WorkersSecurityGroup1
        - Description: "Allow connections from second CNFW interface"
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 65535
          SourceSecurityGroupId: !Ref WorkersSecurityGroup2
        - Description: "Allow connections from third CNFW interface"
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 65535
          SourceSecurityGroupId: !Ref WorkersSecurityGroup3

  WorkersSecurityGroup1:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for the first additional interface of cluster workers.
      VpcId: !Ref VPCID
      Tags:
        - Key: Name
          Value: !Sub "${EKSClusterName}-workers-1"

  WorkersSecurityGroup2:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for the second additional interface of enforcement point workers. That interface is designed to be used as an outside facing interface.
      VpcId: !Ref VPCID
      Tags:
        - Key: Name
          Value: !Sub "${EKSClusterName}-workers-2"
      SecurityGroupIngress:
        - Description: "Allow all ingress connections."
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 0
          CidrIp: 0.0.0.0/0
      SecurityGroupEgress:
        - Description: "Allow all egress connections."
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 0
          CidrIp: 0.0.0.0/0

  WorkersSecurityGroup3:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for the third additional interface of enforcement point workers. That interface is designed to be used as an inside facing interface.
      VpcId: !Ref VPCID
      Tags:
        - Key: Name
          Value: !Sub "${EKSClusterName}-workers-3"

  EFSSecurityGroup:
    Condition: EnableEFS
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group that allows inbound NFS traffic for Amazon EFS mount points
      GroupName: !Sub "${EKSClusterName}-efs"
      VpcId: !Ref VPCID

  EFSSecurityGroupIngress:
    Condition: EnableEFS
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allows inbound NFS traffic from the CIDR of cluster VPC
      GroupId: !Ref EFSSecurityGroup
      SourceSecurityGroupId: !Ref WorkersSecurityGroup
      IpProtocol: tcp
      FromPort: 2049
      ToPort: 2049

  # IAM

  EC2WorkersRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
        - "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
        - "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
      Policies:
        # Allow nodes to signal CF resources from user-data
        - PolicyName: cfn-signal
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - cloudformation:SignalResource
                Resource: "*"
        # Allow route53ingress controller to manage route53 records
        - PolicyName: !Sub "${EKSClusterName}-workers-route53-policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "route53:ChangeResourceRecordSets"
                  - "route53:ListResourceRecordSets"
                Resource: "*"

        # Allow worker to associate EIP
        - PolicyName: !Sub "${EKSClusterName}-workers-eip-policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "ec2:DescribeAddresses"
                  - "ec2:AssociateAddress"
                  - "ec2:AllocateAddress"
                  - "ec2:DescribeNetworkInterfaceAttribute"
                  - "ec2:DescribeNetworkInterfaces"
                  - "ec2:DescribeRegions"
                Resource: "*"

        # Allow worker nodes to manage additional ENIs
        - PolicyName: !Sub "${EKSClusterName}-workers-eni-policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "ec2:CreateTags"
                  - "ec2:DescribeSubnets"
                  - "ec2:AttachNetworkInterface"
                  - "ec2:CreateNetworkInterface"
                  - "ec2:ModifyNetworkInterfaceAttribute"
                Resource: "*"

        # Allow worker nodes to perform CA API calls
        - PolicyName: !Sub "${EKSClusterName}-workers-autoscaler-policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "autoscaling:DescribeAutoScalingGroups"
                  - "autoscaling:DescribeAutoScalingInstances"
                  - "autoscaling:DescribeLaunchConfigurations"
                  - "autoscaling:DescribeTags"
                  - "autoscaling:SetDesiredCapacity"
                  - "autoscaling:TerminateInstanceInAutoScalingGroup"
                  - "ec2:DescribeLaunchTemplateVersions"
                Resource: "*"

        # Allow worker nodes to update S3 buckets created for the cluster
        - PolicyName: !Sub "${EKSClusterName}-workers-s3-policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "s3:PutObject"
                  - "s3:PutObjectAcl"
                  - "s3:GetObject"
                  - "s3:GetObjectAcl"
                  - "s3:DeleteObject"
                  - "s3:ListBucket"
                  - "s3:GetBucketLocation"
                Resource: "*"

        # Allow worker nodes to update Route Tables for ipv4subnetpool integration
        - PolicyName: !Sub "${EKSClusterName}-workers-route-table-policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "ec2:DescribeRouteTables"
                  - "ec2:CreateRoute"
                  - "ec2:ReplaceRoute"
                  - "ec2:DeleteRoute"
                Resource: "*"

  OIDCProviderArn:
    Condition: EnableEFS
    DependsOn: EKSControlPlane
    Type: Custom::CliQuery
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-ResourceReader'
      AwsCliCommand: !Sub
        - 'iam list-open-id-connect-providers --query "OpenIDConnectProviderList[?contains(Arn,''${EKSEndpointID}'')] | [0]"'
        - EKSEndpointID: !Select [ 1, !Split [ "https://" , !Select [ 0, !Split [ "." , !Sub '${EKSControlPlane.Outputs.EKSEndpoint}' ] ] ] ]
      IdField: Arn

  OIDCProviderURL:
    Condition: EnableEFS
    Type: Custom::CliQuery
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-ResourceReader'
      AwsCliCommand: !Sub 'iam get-open-id-connect-provider --open-id-connect-provider-arn ${OIDCProviderArn}'
      IdField: Url

  EFSDriverRole:
    Condition: EnableEFS
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument: !Sub |
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "Federated": "${OIDCProviderArn}"
              },
              "Action": "sts:AssumeRoleWithWebIdentity",
              "Condition": {
                "StringEquals": {
                  "${OIDCProviderURL}:sub": "system:serviceaccount:kube-system:efs-csi-controller-sa"
                }
              }
            }
          ]
        }
      Policies:
        - PolicyName: !Sub "${EKSClusterName}-efs-csi-driver-policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "elasticfilesystem:DescribeAccessPoints"
                  - "elasticfilesystem:DescribeFileSystems"
                Resource: "*"
              - Effect: Allow
                Action:
                  - "elasticfilesystem:CreateAccessPoint"
                Resource: "*"
                Condition:
                  StringLike:
                    "aws:RequestTag/efs.csi.aws.com/cluster": "true"
              - Effect: Allow
                Action:
                  - "elasticfilesystem:DeleteAccessPoint"
                Resource: "*"
                Condition:
                  StringEquals:
                    "aws:ResourceTag/efs.csi.aws.com/cluster": "true"

  # EKS Cluster

  EKSControlPlane:
    Type: AWS::CloudFormation::Stack
    DependsOn: ControlPlaneSecurityGroupIngress
    Properties:
      TemplateURL:
        !Sub
        - 'https://${S3Bucket}.s3.${S3Region}.${AWS::URLSuffix}/${QSS3KeyPrefix}templates/amazon-eks-controlplane.template.yaml'
        - S3Region: !If [ UsingDefaultBucket, !Ref 'AWS::Region', !Ref QSS3BucketRegion ]
          S3Bucket: !If [ UsingDefaultBucket, !Sub '${QSS3BucketName}-${AWS::Region}', !Ref QSS3BucketName ]
      Parameters:
        EKSClusterName: !Ref EKSClusterName
        KubernetesVersion: !Ref KubernetesVersion
        SecurityGroupIds: !Ref ControlPlaneSecurityGroup
        SubnetIds: !Join
          - ","
          - - !Ref PublicSubnet1ID
            - !Ref PublicSubnet2ID
            - !Ref PrivateSubnet1ID
            - !Ref PrivateSubnet2ID
        RoleArn: !Sub "arn:${AWS::Partition}:iam::${AWS::AccountId}:role/eks-quickstart-ControlPlane"
        CustomNodeRole: !Ref EC2WorkersRole
        FunctionRoleArn: !GetAtt IamStack.Outputs.KubernetesAdminRoleArn
        AdditionalEKSAdminUserArn: !Ref EKSAdminUserArn
        AdditionalEKSAdminRoleArn: !Ref AdditionalEKSAdminRoleArn
        EKSPublicAccessEndpoint: "Enabled"
        EKSPrivateAccessEndpoint: "Disabled"
        EKSEncryptSecrets: "Disabled"
        EKSClusterLoggingTypes: !Ref EKSClusterLoggingTypes

  # TODO: remove it altogether in v1.0.1, it was kept for backward compatibility with existing stacks
  AwsAuthConfigMap:
    Type: "Custom::KubeManifest"
    Properties:
      ServiceToken: !GetAtt ApplyKubernetesManifestFunction.Arn
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      # No-op
      Manifest: ""

  # EFS

  EFSFileSystem:
    Condition: EnableEFS
    Type: AWS::EFS::FileSystem
    Properties:
      Encrypted: true
      LifecyclePolicies:
        - TransitionToIA: AFTER_14_DAYS
      FileSystemTags:
        - Key: Name
          Value: !Ref EKSClusterName

  MountTargetAZ1:
    Condition: EnableEFS
    Type: AWS::EFS::MountTarget
    DependsOn: EFSSecurityGroupIngress
    Properties:
      FileSystemId: !Ref EFSFileSystem
      SubnetId: !Ref PrivateSubnet1ID
      SecurityGroups:
        - !Ref EFSSecurityGroup

  MountTargetAZ2:
    Condition: EnableEFS
    Type: AWS::EFS::MountTarget
    DependsOn: EFSSecurityGroupIngress
    Properties:
      FileSystemId: !Ref EFSFileSystem
      SubnetId: !Ref PrivateSubnet2ID
      SecurityGroups:
        - !Ref EFSSecurityGroup

  # Node groups

  NodeInstanceProfile:
    Type: "AWS::IAM::InstanceProfile"
    Properties:
      Path: /
      Roles:
        - !Ref EC2WorkersRole

  CPWorkersGroupUnmanaged:
    Type: "AWS::AutoScaling::AutoScalingGroup"
    DependsOn: [ CalicoCNI ]
    Properties:
      DesiredCapacity: !Ref ControlNodeGroupDesiredSize
      MaxSize: !Ref ControlNodeGroupMaxSize
      MinSize: "1"
      LaunchTemplate:
        LaunchTemplateId: !Ref CPLaunchTemplate
        Version: !GetAtt CPLaunchTemplate.LatestVersionNumber
      Tags:
        - Key: Name
          PropagateAtLaunch: true
          Value: !Sub ${EKSControlPlane.Outputs.EKSName}-cpworkers
        - Key: !Sub kubernetes.io/cluster/${EKSControlPlane.Outputs.EKSName}
          PropagateAtLaunch: true
          Value: owned
        - Key: k8s.io/cluster-autoscaler/enabled
          Value: 'true'
          PropagateAtLaunch: true
        - Key: !Sub 'k8s.io/cluster-autoscaler/${EKSControlPlane.Outputs.EKSName}'
          Value: ''
          PropagateAtLaunch: true
      VPCZoneIdentifier:
        - !Ref PublicSubnet1ID
        - !Ref PublicSubnet2ID

  CPLaunchTemplate:
    Type: "AWS::EC2::LaunchTemplate"
    Properties:
      LaunchTemplateName: !Sub "${EKSClusterName}-cpworkers"
      LaunchTemplateData:
        IamInstanceProfile:
          Arn: !GetAtt NodeInstanceProfile.Arn
        KeyName: !Ref KeyPairName
        ImageId: !FindInMap [ WorkerAMIMap, !Ref AWS::Region, "118" ]
        InstanceType: !FindInMap [ ConfigurationTierMap, !Ref ControllerConfigurationTier, instanceType ]
        NetworkInterfaces:
          - DeviceIndex: 0
            AssociatePublicIpAddress: true
            DeleteOnTermination: true
            Groups:
              - !Ref WorkersSecurityGroup
              - !Ref WorkersSecurityGroup0
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${EKSClusterName}-cpworker"
              - Key: k8s.io/cluster-autoscaler/enabled
                Value: "true"
              - Key: !Sub "k8s.io/cluster-autoscaler/${EKSClusterName}"
                Value: owned
        UserData: !Base64
          "Fn::Sub": |
            Content-Type: multipart/mixed; boundary="MIMEBOUNDARY"
            MIME-Version: 1.0

            --MIMEBOUNDARY
            Content-Type: text/cloud-config; charset="us-ascii"
            MIME-Version: 1.0
            Content-Transfer-Encoding: 7bit
            Content-Disposition: attachment; filename="cloud-config.txt"

            #cloud-config
            cloud_final_modules:
            - [scripts-user, always]

            --MIMEBOUNDARY
            Content-Disposition: attachment; filename="03-configure-instance.sh"
            Content-Transfer-Encoding: 7bit
            Content-Type: text/x-shellscript
            Mime-Version: 1.0

            #!/bin/bash

            echo "setting hugepages and doing modprobe"

            swapoff -a
            cd /etc && sudo sed -i.backup '/swap/s/^/#/g' fstab && cd ~

            HUGEPGSZ=`cat /proc/meminfo  | grep Hugepagesize | cut -d : -f 2 | tr -d ' '`

            clear_huge_pages()
            {
                    echo > .echo_tmp
                    for d in /sys/devices/system/node/node? ; do
                            echo "echo 0 > $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages" >> .echo_tmp
                    done
                    echo "Removing currently reserved hugepages"
                    chmod 755 .echo_tmp
                    sh .echo_tmp
                    rm -f .echo_tmp

                    remove_mnt_huge
            }

            remove_mnt_huge()
            {
                    echo "Unmounting /mnt/huge and removing directory"
                    grep -s '/mnt/huge' /proc/mounts > /dev/null
                    if [[ $? -eq 0  && -d /mnt/huge ]] ; then
                            umount /mnt/huge
                            rm -R /mnt/huge
                    fi
            }

            set_numa_pages()
            {
                    clear_huge_pages
                    NEW_PAGES=$1
                    echo "The new page is $NEW_PAGES"

                    echo "#! /bin/bash" > .echo_tmp
                    for d in /sys/devices/system/node/node? ; do
                            node=$(basename $d)
                            echo "Setting $NEW_PAGES pages for $node"
                            if [ ! -e $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages ]; then
                              echo "echo $NEW_PAGES > $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages" >> .echo_tmp
                            else
                              temp=$(cat $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages)
                              if [ "$temp" != "$NEW_PAGES" ]; then
                                  echo "echo $NEW_PAGES > $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages" >> .echo_tmp
                              fi
                            fi
                    done
                    chmod 755 .echo_tmp
                    echo "Reserving hugepages"
                    sh ./.echo_tmp
                    rm -f .echo_tmp

                    create_mnt_huge
            }

            create_mnt_huge()
            {
                    echo "Creating /mnt/huge and mounting as hugetlbfs"
                    mkdir -p /mnt/huge

                    grep '/mnt/huge' /proc/mounts
                    if [ $? -ne 0 ] ; then
                            echo "trying to mount"
                            mount -t hugetlbfs nodev /mnt/huge
                            echo "finished mounting hugetlbfs"
                    fi
                    echo "done mounting things"
            }

            display_current_setting()
            {
                    for d in /sys/devices/system/node/node? ; do
                            node=$(basename $d)
                            echo "huge pages for $node: "
                            cat $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages
                    done
            }

            set_numa_pages 128

            modprobe igb_uio wc_activate=1
            modprobe dm_crypt

            # need to create tap_nlp ahead of time and set rp_filter off across the board.
            # NOTE: could (should?) be moved into init container along with interface attachment
            #       logic, but that may require some permissions and stuff.
            ip tuntap add tap_nlp mode tap
            ip link set tap_nlp up
            echo 0 >/proc/sys/net/ipv4/conf/all/rp_filter
            echo 0 >/proc/sys/net/ipv4/conf/tap_nlp/rp_filter

            # override default docker ulimits according to STO's recommendation
            sudo bash -c "cat > /etc/docker/daemon.json" <<EOF
            {
                "log-driver": "json-file",
                "log-opts": {
                    "max-size": "20m",
                    "max-file": "3",
                    "compress": "true"
                },
                "default-ulimits": {
                    "nofile": {
                        "Hard": 1024,
                        "Soft": 1024,
                        "Name": "nofile"
                    },
                    "nproc": {
                        "Hard": 4096,
                        "Soft": 4096,
                        "Name": "nproc"
                    }
                }
            }
            EOF
            # restarting Docker daemon for new ulimits to be applied
            sudo systemctl restart docker

            --MIMEBOUNDARY
            Content-Disposition: attachment; filename="06-install-ecr-login-helper.sh"
            Content-Transfer-Encoding: 7bit
            Content-Type: text/x-shellscript
            Mime-Version: 1.0

            #!/bin/bash
            set -o xtrace

            sudo apt install amazon-ecr-credential-helper
            sudo mkdir -p /root/.docker
            sudo bash -c "cat > /root/.docker/config.json" <<EOF
            {
                "credsStore": "ecr-login"
            }
            EOF
            echo "export DOCKER_CONTENT_TRUST=0" | sudo tee -a /etc/environment


            --MIMEBOUNDARY
            Content-Disposition: attachment; filename="99-register-node.sh"
            Content-Transfer-Encoding: 7bit
            Content-Type: text/x-shellscript
            Mime-Version: 1.0

            #!/bin/bash
            set -o xtrace

            /etc/eks/bootstrap.sh ${EKSControlPlane.Outputs.EKSName} \
                     --kubelet-extra-args '--node-labels=sfcn.cisco.com/nodetype=configuration' \
                     --b64-cluster-ca ${EKSControlPlane.Outputs.CAData} \
                     --apiserver-endpoint ${EKSControlPlane.Outputs.EKSEndpoint} \
                     --use-max-pods true

  EPWorkersGroupUnmanaged:
    Type: "AWS::AutoScaling::AutoScalingGroup"
    DependsOn: [ CalicoCNI ]
    Properties:
      DesiredCapacity: !Ref EnforcerNodeGroupDesiredSize
      MaxSize: !Ref EnforcerNodeGroupMaxSize
      MinSize: "1"
      LaunchTemplate:
        LaunchTemplateId: !Ref EPLaunchTemplate
        Version: !GetAtt EPLaunchTemplate.LatestVersionNumber
      Tags:
        - Key: Name
          PropagateAtLaunch: true
          Value: !Sub ${EKSControlPlane.Outputs.EKSName}-epworkers
        - Key: !Sub kubernetes.io/cluster/${EKSControlPlane.Outputs.EKSName}
          PropagateAtLaunch: true
          Value: owned
        - Key: k8s.io/cluster-autoscaler/enabled
          Value: 'true'
          PropagateAtLaunch: true
        - Key: !Sub 'k8s.io/cluster-autoscaler/${EKSControlPlane.Outputs.EKSName}'
          Value: ''
          PropagateAtLaunch: true
      VPCZoneIdentifier:
        - !Ref PublicSubnet1ID
        - !Ref PublicSubnet2ID

  EPLaunchTemplate:
    Type: "AWS::EC2::LaunchTemplate"
    Properties:
      LaunchTemplateName: !Sub "${EKSClusterName}-epworkers"
      LaunchTemplateData:
        IamInstanceProfile:
          Arn: !GetAtt NodeInstanceProfile.Arn
        KeyName: !Ref KeyPairName
        ImageId: !FindInMap [ WorkerAMIMap, !Ref AWS::Region, "118" ]
        InstanceType: !FindInMap [ ConfigurationTierMap, !Ref EnforcerConfigurationTier, instanceType ]
        NetworkInterfaces:
          - DeviceIndex: 0
            AssociatePublicIpAddress: true
            DeleteOnTermination: true
            Groups:
              - !Ref WorkersSecurityGroup
              - !Ref WorkersSecurityGroup0
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${EKSClusterName}-epworker"
              - Key: k8s.io/cluster-autoscaler/enabled
                Value: "true"
              - Key: !Sub "k8s.io/cluster-autoscaler/${EKSClusterName}"
                Value: owned
              - Key: k8s.io/cluster-autoscaler/node-template/resources/hugepages-2Mi
                Value: 64Mi
              - Key: !Sub kubernetes.io/cluster/${EKSControlPlane.Outputs.EKSName}
                Value: owned
        UserData: !Base64
          "Fn::Sub":
            - |
              Content-Type: multipart/mixed; boundary="MIMEBOUNDARY"
              MIME-Version: 1.0

              --MIMEBOUNDARY
              Content-Type: text/cloud-config; charset="us-ascii"
              MIME-Version: 1.0
              Content-Transfer-Encoding: 7bit
              Content-Disposition: attachment; filename="cloud-config.txt"

              #cloud-config
              cloud_final_modules:
              - [scripts-user, always]

              --MIMEBOUNDARY
              Content-Disposition: attachment; filename="00-override_pragma.sh"
              Content-Transfer-Encoding: 7bit
              Content-Type: text/x-shellscript
              Mime-Version: 1.0

              #!/bin/bash
              # rcS.common tries to treat user-data as day0 configuration,
              # following pragma disables that behavior
              # AWSOVERRIDE


              --MIMEBOUNDARY
              Content-Disposition: attachment; filename="04-install-deps.sh"
              Content-Transfer-Encoding: 7bit
              Content-Type: text/x-shellscript
              Mime-Version: 1.0

              #!/bin/bash

              echo 'debconf debconf/frontend select Noninteractive' | debconf-set-selections
              apt-get update -yq
              apt-get upgrade -yq
              apt-get install -yq jq software-properties-common dpdk dpdk-igb-uio-dkms ec2-instance-connect
              apt-cache show hugepages && sudo apt-get install -yq hugepages || true
              sudo apt install amazon-ecr-credential-helper
              sudo mkdir -p /root/.docker
              sudo bash -c "cat > /root/.docker/config.json" <<EOF
              {
                  "credsStore": "ecr-login"
              }
              EOF
              echo "export DOCKER_CONTENT_TRUST=0" | sudo tee -a /etc/environment


              --MIMEBOUNDARY
              Content-Disposition: attachment; filename="01-attach-nics.sh"
              Content-Transfer-Encoding: 7bit
              Content-Type: text/x-shellscript
              Mime-Version: 1.0

              #!/bin/bash

              export AWS_DEFAULT_REGION=${AWS::Region}
              export AWS_MAX_ATTEMPTS=20
              INSTANCE_ID=$(curl -sS http://169.254.169.254/latest/meta-data/instance-id)
              AZ=$(curl -sS http://169.254.169.254/latest/meta-data/placement/availability-zone)
              OUTSIDE_SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=${VPCID}" --filters "Name=tag-key,Values=Name" "Name=tag-value,Values=${EKSClusterName}_outside_$AZ" --output text --query 'Subnets[0].SubnetId')

              # make sure EIP is really attached with retry and runtime allocation
              attach_eip() {
                local pool=sfcn
                local retries=5
                local count=0
                local eni=$1
                while [ $count -lt $retries ]; do
                  # fetch available EIPs from the pool
                  local eips=$(aws ec2 describe-addresses --filters "Name=tag-key,Values=Pool" "Name=tag-value,Values=$pool" --query 'Addresses[?AssociationId==null].{AllocationId:AllocationId}' --output text)
                  echo "$(echo $eips | wc -l) EIPs available in the pool"
                  # check if something is actualy available
                  local eip
                  if [ -z "$eips" ]; then
                    # allocate, since pool seems to be empty
                    eip=$(aws ec2 allocate-address --query 'AllocationId' --output text)
                    aws ec2 create-tags --resources=$eip --tags Key=Pool,Value=$pool
                    echo "added $eip to the pool"
                  else
                    # pick one at random from the pool
                    eip=$(echo "$eips" | sort -R | head -n1)
                    echo "picked $eip from the pool"
                  fi
                  echo "associating $eip to $eni"
                  # try attaching, break loop if successful
                  aws ec2 associate-address --no-allow-reassociation --allocation-id $eip --network-interface-id $eni && break
                  count=$(($count + 1))
                  # random jitter just in case
                  sleep $((RANDOM % 4))
                done
              }

              provision_mgmt() {
                # mgmt
                DEFAULT_MAC=$(curl -sS http://169.254.169.254/latest/meta-data/network/interfaces/macs)
                SUBNET_ID=$(curl -sS http://169.254.169.254/latest/meta-data/network/interfaces/macs/$DEFAULT_MAC/subnet-id)
                ENI_ID=$(aws ec2 create-network-interface --subnet $SUBNET_ID --query 'NetworkInterface.NetworkInterfaceId' --output text)
                ATTACHMENT_ID=$(aws ec2 attach-network-interface --network-interface-id $ENI_ID --instance-id $INSTANCE_ID --device-index 1 --output text)
                aws ec2 modify-network-interface-attribute --network-interface-id $ENI_ID --attachment AttachmentId=$ATTACHMENT_ID,DeleteOnTermination=true
                aws ec2 modify-network-interface-attribute --network-interface-id "$ENI_ID" --source-dest-check='{"Value": false}'

                aws ec2 modify-network-interface-attribute \
                --network-interface-id $ENI_ID \
                --groups "${WorkersSecurityGroup1}" "${DefaultSecurityGroupId}"
              }

              provision_outside() {
                OUTSIDE_ENI_ID=$(aws ec2 create-network-interface --subnet $OUTSIDE_SUBNET_ID --query 'NetworkInterface.NetworkInterfaceId' --output text)
                ATTACHMENT_ID=$(aws ec2 attach-network-interface --network-interface-id $OUTSIDE_ENI_ID --instance-id $INSTANCE_ID --device-index 2 --output text)
                aws ec2 modify-network-interface-attribute --network-interface-id $OUTSIDE_ENI_ID --attachment AttachmentId=$ATTACHMENT_ID,DeleteOnTermination=true
                aws ec2 modify-network-interface-attribute --network-interface-id "$OUTSIDE_ENI_ID" --source-dest-check='{"Value": false}'

                if [[ "${EIPAttachmentMode}" == "outside" || "${EIPAttachmentMode}" == "both" ]]; then
                  attach_eip $OUTSIDE_ENI_ID
                fi

                aws ec2 modify-network-interface-attribute \
                --network-interface-id $OUTSIDE_ENI_ID \
                --groups "${WorkersSecurityGroup2}" "${DefaultSecurityGroupId}"
              }

              provision_inside() {
                INSIDE_SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=${VPCID}" --filters "Name=tag-key,Values=Name" "Name=tag-value,Values=${EKSClusterName}_inside_$AZ" --output text --query 'Subnets[0].SubnetId')
                INSIDE_ENI_ID=$(aws ec2 create-network-interface --subnet $INSIDE_SUBNET_ID --query 'NetworkInterface.NetworkInterfaceId' --output text)
                ATTACHMENT_ID=$(aws ec2 attach-network-interface --network-interface-id $INSIDE_ENI_ID --instance-id $INSTANCE_ID --device-index 3 --output text)
                aws ec2 modify-network-interface-attribute --network-interface-id $INSIDE_ENI_ID --attachment AttachmentId=$ATTACHMENT_ID,DeleteOnTermination=true
                aws ec2 modify-network-interface-attribute --network-interface-id "$INSIDE_ENI_ID" --source-dest-check='{"Value": false}'

                if [[ "${EIPAttachmentMode}" == "inside" || "${EIPAttachmentMode}" == "both" ]]; then
                  attach_eip $INSIDE_ENI_ID
                fi

                aws ec2 modify-network-interface-attribute \
                --network-interface-id $INSIDE_ENI_ID \
                --groups "${WorkersSecurityGroup3}" "${DefaultSecurityGroupId}"
              }

              OUTSIDE_INTF_SIZE=$(aws ec2 describe-network-interfaces --filters Name=subnet-id,Values=$OUTSIDE_SUBNET_ID Name=attachment.instance-id,Values=$INSTANCE_ID --query 'length(NetworkInterfaces)')

              if [[ $OUTSIDE_INTF_SIZE == 0 ]]; then
                echo 'provisioning interfaces'
                provision_mgmt
                provision_inside
                provision_outside
              fi


              --MIMEBOUNDARY
              Content-Disposition: attachment; filename="03-configure-instance.sh"
              Content-Transfer-Encoding: 7bit
              Content-Type: text/x-shellscript
              Mime-Version: 1.0

              #!/bin/bash

              echo "setting hugepages and doing modprobe"

              swapoff -a
              cd /etc && sudo sed -i.backup '/swap/s/^/#/g' fstab && cd ~

              HUGEPGSZ=`cat /proc/meminfo  | grep Hugepagesize | cut -d : -f 2 | tr -d ' '`

              clear_huge_pages()
              {
                      echo > .echo_tmp
                      for d in /sys/devices/system/node/node? ; do
                              echo "echo 0 > $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages" >> .echo_tmp
                      done
                      echo "Removing currently reserved hugepages"
                      chmod 755 .echo_tmp
                      sh .echo_tmp
                      rm -f .echo_tmp

                      remove_mnt_huge
              }

              remove_mnt_huge()
              {
                      echo "Unmounting /mnt/huge and removing directory"
                      grep -s '/mnt/huge' /proc/mounts > /dev/null
                      if [[ $? -eq 0  && -d /mnt/huge ]] ; then
                              umount /mnt/huge
                              rm -R /mnt/huge
                      fi
              }

              set_numa_pages()
              {
                      clear_huge_pages
                      NEW_PAGES=$1
                      echo "The new page is $NEW_PAGES"

                      echo "#! /bin/bash" > .echo_tmp
                      for d in /sys/devices/system/node/node? ; do
                              node=$(basename $d)
                              echo "Setting $NEW_PAGES pages for $node"
                              if [ ! -e $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages ]; then
                                echo "echo $NEW_PAGES > $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages" >> .echo_tmp
                              else
                                temp=$(cat $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages)
                                if [ "$temp" != "$NEW_PAGES" ]; then
                                    echo "echo $NEW_PAGES > $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages" >> .echo_tmp
                                fi
                              fi
                      done
                      chmod 755 .echo_tmp
                      echo "Reserving hugepages"
                      sh ./.echo_tmp
                      rm -f .echo_tmp

                      create_mnt_huge
              }

              create_mnt_huge()
              {
                      echo "Creating /mnt/huge and mounting as hugetlbfs"
                      mkdir -p /mnt/huge

                      grep '/mnt/huge' /proc/mounts
                      if [ $? -ne 0 ] ; then
                              echo "trying to mount"
                              mount -t hugetlbfs nodev /mnt/huge
                              echo "finished mounting hugetlbfs"
                      fi
                      echo "done mounting things"
              }

              display_current_setting()
              {
                      for d in /sys/devices/system/node/node? ; do
                              node=$(basename $d)
                              echo "huge pages for $node: "
                              cat $d/hugepages/hugepages-${!HUGEPGSZ}/nr_hugepages
                      done
              }

              set_numa_pages ${HugePagesSize}

              modprobe igb_uio wc_activate=1
              modprobe dm_crypt

              # need to create tap_nlp ahead of time and set rp_filter off across the board.
              # NOTE: could (should?) be moved into init container along with interface attachment
              #       logic, but that may require some permissions and stuff.
              ip tuntap add tap_nlp mode tap
              ip link set tap_nlp up
              echo 0 >/proc/sys/net/ipv4/conf/all/rp_filter
              echo 0 >/proc/sys/net/ipv4/conf/tap_nlp/rp_filter

              # override default docker ulimits according to STO's recommendation
              sudo bash -c "cat > /etc/docker/daemon.json" <<EOF
              {
                  "log-driver": "json-file",
                  "log-opts": {
                      "max-size": "20m",
                      "max-file": "3",
                      "compress": "true"
                  },
                  "default-ulimits": {
                      "nofile": {
                          "Hard": 1024,
                          "Soft": 1024,
                          "Name": "nofile"
                      },
                      "nproc": {
                          "Hard": 4096,
                          "Soft": 4096,
                          "Name": "nproc"
                      }
                  }
              }
              EOF
              # restarting Docker daemon for new ulimits to be applied
              sudo systemctl restart docker

              --MIMEBOUNDARY
              Content-Disposition: attachment; filename="05-create-core-helper.sh"
              Content-Transfer-Encoding: 7bit
              Content-Type: text/x-shellscript
              Mime-Version: 1.0

              #!/bin/bash

              [ ! -f /mnt/coredump_helper ] && cat <<EOF > /mnt/coredump_helper
              #!/usr/bin/python3

              import sys
              import os
              import os.path
              import subprocess
              import io
              import grp
              import logging
              from datetime import datetime

              DEFAULT_LIMIT=41943040 # 40 MB default
              MULTIPLIER = 1024
              LOG_FILE= '/var/log/core.log'

              def parsePodInfo(pid):
                  env = {}
                  with open('/proc/%s/environ' % pid) as fd:
                      for envspec in fd.read().split('\000'):
                          if not envspec:
                              continue
                          try:
                              varname, varval = envspec.split('=', 1)
                              env[varname] = varval
                          except Exception:
                              continue

                  return env

              def getSize(start_path = '.'):
                  total_size = 0
                  for dirpath, dirnames, filenames in os.walk(start_path):
                      for f in filenames:
                          fp = os.path.join(dirpath, f)
                          # skip if it is symbolic link
                          if not os.path.islink(fp):
                              total_size += os.path.getsize(fp)

                  return total_size

              def writeCoreDump(env):

                  coreLimit = DEFAULT_LIMIT

                  now = datetime.fromtimestamp(int(timeStamp))
                  date = now.strftime("%Y%m%d")
                  time = now.strftime("%H:%M:%S")

                  key = "POD_NAME"
                  if key in env:
                      podName = env["POD_NAME"]
                      podNamespace = env["POD_NAMESPACE"]
                      corePath = env["COREDUMP_PATH"]
                      coreLimitString = env["CORE_SIZE_LIMIT"].rstrip('MB')
                      coreLimit = MULTIPLIER * MULTIPLIER * int(coreLimitString)
                      logging.info("Core detectected in EP pod: %s ,Namespace: %s , dumping to path: %s",
                                  podName, podNamespace, corePath)
                  else:
                      podName = "HOST"
                      podNamespace = "NONE"
                      corePath = "/tmp/dumps/"
                      logging.info("Core detectected in Host, dumping core to %s",corePath)

                  os.makedirs(corePath,exist_ok=True)

                  coreFile = os.path.join(corePath, '-'.join(["core",process_name, podNamespace, podName,date,time, pid, signum]) + ".gz")

                  if getSize(corePath) < coreLimit:
                      core = open(coreFile, 'w')
                      p = subprocess.Popen("gzip", shell=True, stdin=sys.stdin, stdout=core, stderr=core)
                      core.close()
                      logging.info("Successfully dumped core to %s", coreFile)
                  else:
                      logging.error("Not enough space left in  %s. Core will not be generated.",corePath)

                  # Start of the script
              try:
                  logging.basicConfig(filename=LOG_FILE,level=logging.DEBUG)

                  if len(sys.argv) != 5:
                      logging.info('Usage: %s <process_name> <pid> <signal number> <time stamp>' % sys.argv[0])
                      logging.info('The core dump is read from stdin.')
                      sys.exit(1)

                  (process_name, pid, signum, timeStamp) = sys.argv[1:5]

                  env = parsePodInfo(pid)

                  writeCoreDump(env)

              except Exception as err:

                  logging.error("Error generating Core Dump : %s", err)

              EOF

              sudo chmod +x /mnt/coredump_helper
              # disabling the default Apport service intercepting crashes in order to overwrite 'kernel.core_pattern'
              sudo systemctl disable apport.service
              sysctl -w kernel.core_pattern="|/mnt/coredump_helper %e %p %s %t"


              --MIMEBOUNDARY
              Content-Disposition: attachment; filename="99-register-node.sh"
              Content-Transfer-Encoding: 7bit
              Content-Type: text/x-shellscript
              Mime-Version: 1.0

              #!/bin/bash
              set -o xtrace

              /etc/eks/bootstrap.sh ${EKSControlPlane.Outputs.EKSName} \
                       --b64-cluster-ca ${EKSControlPlane.Outputs.CAData} \
                       --apiserver-endpoint ${EKSControlPlane.Outputs.EKSEndpoint} \
                       --use-max-pods true \
                       --kubelet-extra-args '--register-with-taints=cisco-nodetype=enforcement:NoSchedule --node-labels=sfcn.cisco.com/nodetype=enforcement'

            - HugePagesSize: !FindInMap [ ConfigurationTierMap, !Ref EnforcerConfigurationTier, hugePagesSize ]


  # Custom Functions

  GetKubectlLayerArn:
    Type: Custom::CliQuery
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-ResourceReader'
      AwsCliCommand: lambda list-layer-versions --layer-name eks-quickstart-Kubectl --query 'max_by(LayerVersions, &Version)'
      IdField: 'LayerVersionArn'

  GetCrhelperLayerArn:
    Type: Custom::CliQuery
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-ResourceReader'
      AwsCliCommand: lambda list-layer-versions --layer-name eks-quickstart-Crhelper --query 'max_by(LayerVersions, &Version)'
      IdField: 'LayerVersionArn'

  GetAwsCliLayerArn:
    Type: Custom::CliQuery
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-ResourceReader'
      AwsCliCommand: lambda list-layer-versions --layer-name eks-quickstart-AwsCli --query 'max_by(LayerVersions, &Version)'
      IdField: 'LayerVersionArn'

  CleanupSecurityGroupDependencies:
    Type: Custom::Cleanup
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-CleanupSecurityGroupDependencies'
      Region: !Ref "AWS::Region"
      SecurityGroups:
        - !Ref ControlPlaneSecurityGroup

  CleanupLambdas:
    Type: Custom::LambdaCleanup
    DependsOn: CleanupSecurityGroupDependencies
    Properties:
      ServiceToken: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:eks-quickstart-CleanupLambdas'
      SecurityGroupId: !GetAtt ControlPlaneSecurityGroup.GroupId

  KubectlFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Timeout: 60
      Runtime: python3.7
      Handler: index.handler
      Role: !GetAtt IamStack.Outputs.KubernetesAdminRoleArn
      Layers: [ !Ref GetKubectlLayerArn, !Ref GetCrhelperLayerArn, !Ref GetAwsCliLayerArn ]
      Code:
        ZipFile: |
          import logging
          import subprocess
          import shlex
          import cfnresponse
          import base64

          def run_command(command):
              try:
                  print("executing command: %s" % command)
                  output = subprocess.check_output(shlex.split(command), stderr=subprocess.STDOUT).decode("utf-8")
                  print(output)
              except subprocess.CalledProcessError as exc:
                  print("Command failed with exit code %s, stderr: %s" % (exc.returncode, exc.output.decode("utf-8")))
                  raise Exception(exc.output.decode("utf-8"))
              return output

          def create_kubeconfig(cluster_name):
              run_command(f"aws eks update-kubeconfig --name {cluster_name} --alias {cluster_name}")
              run_command(f"kubectl config use-context {cluster_name}")

          def handler(event, context):
              print(event)
              status = cfnresponse.SUCCESS
              data = {}
              try:
                  if event['RequestType'] == 'Create' or event['RequestType'] == 'Update':
                      create_kubeconfig(event['ResourceProperties']['ClusterName'])
                      command = event['ResourceProperties']['Command']
                      data["Output"] = run_command(f"kubectl {command}")
                      print(data["Output"])
                      if event['ResourceProperties'].get('Decode') == 'base64':
                          data["Output"] = base64.b64decode(data["Output"]).decode('utf-8')
              except Exception:
                  logging.error('Unhandled exception', exc_info=True)
                  status = cfnresponse.FAILED
              finally:
                  cfnresponse.send(event, context, status, data, event.get('PhysicalResourceId', context.log_stream_name))
      Environment:
        Variables:
          KUBECONFIG: /tmp/.kube/config
          NO_PROXY: !Sub '${VPCCIDR},localhost,127.0.0.1,169.254.169.254,.internal'
      VpcConfig:
        SecurityGroupIds: [ !Ref ControlPlaneSecurityGroup ]
        SubnetIds:
          - !Ref PrivateSubnet1ID
          - !Ref PrivateSubnet2ID

  # AWSQS::Kubernetes::Resource cannot apply YAMLs that have multiple resources separated by ---.
  # Hence, this custom function is declared.
  # For single resources AWSQS::Kubernetes::Resource is still preferable.
  ApplyKubernetesManifestFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Timeout: 60
      Runtime: python3.7
      Handler: index.handler
      Role: !GetAtt IamStack.Outputs.KubernetesAdminRoleArn
      Layers: [ !Ref GetKubectlLayerArn, !Ref GetCrhelperLayerArn, !Ref GetAwsCliLayerArn ]
      Code:
        ZipFile: |
          import logging
          import subprocess
          import shlex
          import cfnresponse
          import requests

          def run_command(command):
              try:
                  print("executing command: %s" % command)
                  output = subprocess.check_output(shlex.split(command), stderr=subprocess.STDOUT).decode("utf-8")
                  print(output)
              except subprocess.CalledProcessError as exc:
                  print("Command failed with exit code %s, stderr: %s" % (exc.returncode, exc.output.decode("utf-8")))
                  raise Exception(exc.output.decode("utf-8"))
              return output

          def create_kubeconfig(cluster_name):
              run_command(f"aws eks update-kubeconfig --name {cluster_name} --alias {cluster_name}")
              run_command(f"kubectl config use-context {cluster_name}")

          def http_get(url):
              try:
                  response = requests.get(url)
              except requests.exceptions.RequestException as e:
                  raise RuntimeError(f"Failed to fetch CustomValueYaml url {url}: {e}")
              if response.status_code != 200:
                  raise RuntimeError(
                      f"Failed to fetch CustomValueYaml url {url}: [{response.status_code}] "
                      f"{response.reason}"
                  )
              return response.text

          def handler(event, context):
              print(event)
              status = cfnresponse.SUCCESS
              try:
                  manifest_file = '/tmp/manifest.json'
                  manifest_text = http_get(event['ResourceProperties']["Url"]) \
                    if event['ResourceProperties'].get("Url") \
                    else event['ResourceProperties']["Manifest"]
                  if not manifest_text:
                      print("Manifest text is empty, exiting...")
                      return
                  with open(manifest_file, 'w') as f:
                      f.write(manifest_text)
                  create_kubeconfig(event['ResourceProperties']['ClusterName'])

                  if event['RequestType'] == 'Create' or event['RequestType'] == 'Update':
                      outp = run_command(f"kubectl apply -f {manifest_file}")
                      print(outp)
                  if event['RequestType'] == 'Delete':
                      outp = run_command(f"kubectl delete -f {manifest_file} --ignore-not-found")
                      print(outp)
              except Exception:
                  logging.error('Unhandled exception', exc_info=True)
                  status = cfnresponse.FAILED
              finally:
                  cfnresponse.send(event, context, status, {}, event.get('PhysicalResourceId', context.log_stream_name))
      Environment:
        Variables:
          KUBECONFIG: /tmp/.kube/config
          NO_PROXY: !Sub '${VPCCIDR},localhost,127.0.0.1,169.254.169.254,.internal'
      VpcConfig:
        SecurityGroupIds: [ !Ref ControlPlaneSecurityGroup ]
        SubnetIds:
          - !Ref PrivateSubnet1ID
          - !Ref PrivateSubnet2ID

  # Kubernetes

  EFSDriver:
    Condition: EnableEFS
    Type: "AWSQS::Kubernetes::Helm"
    Properties:
      ClusterID: !GetAtt EKSControlPlane.Outputs.EKSName
      Name: "aws-efs-csi-driver"
      Namespace: "kube-system"
      Chart: "aws-efs-csi-driver"
      Repository: "https://kubernetes-sigs.github.io/aws-efs-csi-driver"
      Version: "1.2.2"
      Values:
        "serviceAccount.controller.annotations.eks\\.amazonaws\\.com/role-arn": !GetAtt EFSDriverRole.Arn

  # Storage class can be created by the aws-efs-csi-driver helm chart, but atm it does not support
  # setting reclaimPolicy: Retain, hence defining StorageClass separately.
  EFSStorageClass:
    Condition: EnableEFS
    DependsOn: EFSDriver
    Type: "AWSQS::Kubernetes::Resource"
    Properties:
      Namespace: ""
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      Manifest: !Sub |
        kind: StorageClass
        apiVersion: storage.k8s.io/v1
        metadata:
          name: sfcn-sc
        provisioner: efs.csi.aws.com
        reclaimPolicy: Retain
        mountOptions: [ tls ]
        parameters:
          provisioningMode: efs-ap
          fileSystemId: ${EFSFileSystem}
          directoryPerms: "700"
          basePath: /sfcn

  DeleteAwsCni:
    Type: "Custom::Kubectl"
    Properties:
      ServiceToken: !GetAtt KubectlFunction.Arn
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      Command: "delete daemonset/aws-node --namespace kube-system --ignore-not-found"

  CalicoCNI:
    Type: "Custom::KubeManifest"
    DependsOn: DeleteAwsCni
    Properties:
      ServiceToken: !GetAtt ApplyKubernetesManifestFunction.Arn
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      Url: https://docs.projectcalico.org/manifests/calico-vxlan.yaml

  CalicoCNIRouteRefreshEnv:
    Type: "Custom::Kubectl"
    DependsOn: CalicoCNI
    Properties:
      ServiceToken: !GetAtt KubectlFunction.Arn
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      # In order for CNFW to manipulate the ip routing rule table, we have to disable route refresh interval.
      Command: "set env daemonset/calico-node -n kube-system FELIX_ROUTEREFRESHINTERVAL='0'"

  MetricsServer:
    Type: "AWSQS::Kubernetes::Resource"
    DependsOn: [ CalicoCNI, CleanupLambdas ]
    Properties:
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      Namespace: kube-system
      Url: https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml

  MetricsServerHostNetwork:
    Type: "Custom::Kubectl"
    DependsOn: MetricsServer
    Properties:
      ServiceToken: !GetAtt KubectlFunction.Arn
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      Command: |-
        patch deployment/metrics-server -n kube-system --patch '{"spec": {"template": {"spec": {"hostNetwork": true, "dnsPolicy": "ClusterFirstWithHostNet"}}}}'

  ClusterAutoscaler:
    Type: "AWSQS::Kubernetes::Helm"
    DependsOn: [ EPWorkersGroupUnmanaged, CPWorkersGroupUnmanaged ]
    Properties:
      ClusterID: !GetAtt EKSControlPlane.Outputs.EKSName
      Name: "autoscaler"
      Namespace: "kube-system"
      Chart: "cluster-autoscaler"
      Repository: "https://kubernetes.github.io/autoscaler"
      Version: "9.3.0"
      Values:
        "nodeSelector.sfcn\\.cisco\\.com/nodetype": "configuration"
        "autoDiscovery.clusterName": !GetAtt EKSControlPlane.Outputs.EKSName
        "extraArgs.skip-nodes-with-system-pods": "false"
        "extraArgs.skip-nodes-with-local-storage": "false"
        "extraArgs.scale-down-unneeded-time": "5m"
        "awsRegion": !Ref AWS::Region

  CertManager:
    Type: "AWSQS::Kubernetes::Helm"
    DependsOn: ClusterAutoscaler
    Properties:
      ClusterID: !GetAtt EKSControlPlane.Outputs.EKSName
      Name: "cert-manager"
      Namespace: "cert-manager"
      Chart: "cert-manager"
      Repository: "https://charts.jetstack.io"
      Version: "v1.2.0"
      Values:
        "nodeSelector.sfcn\\.cisco\\.com/nodetype": "configuration"
        "webhook.nodeSelector.sfcn\\.cisco\\.com/nodetype": "configuration"
        "cainjector.nodeSelector.sfcn\\.cisco\\.com/nodetype": "configuration"
        "installCRDs": "true"
        "webhook.hostNetwork": "true"
        "webhook.securePort": "10251"
        "podAnnotations.seccomp\\.security\\.alpha\\.kubernetes\\.io/pod": "runtime/default"
        "containerSecurityContext.runAsNonRoot": "true"
        "containerSecurityContext.allowPrivilegeEscalation": "false"
        "resources.limits.cpu": "100m"
        "resources.limits.memory": "50Mi"
        "resources.requests.cpu": "25m"
        "resources.requests.memory": "30Mi"
        "cainjector.podAnnotations.seccomp\\.security\\.alpha\\.kubernetes\\.io/pod": "runtime/default"
        "cainjector.containerSecurityContext.runAsNonRoot": "true"
        "cainjector.containerSecurityContext.allowPrivilegeEscalation": "false"
        "cainjector.resources.limits.cpu": "100m"
        "cainjector.resources.limits.memory": "100Mi"
        "cainjector.resources.requests.cpu": "25m"
        "cainjector.resources.requests.memory": "50Mi"
        "webhook.podAnnotations.seccomp\\.security\\.alpha\\.kubernetes\\.io/pod": "runtime/default"
        "webhook.containerSecurityContext.runAsNonRoot": "true"
        "webhook.containerSecurityContext.allowPrivilegeEscalation": "false"
        "webhook.resources.limits.cpu": "100m"
        "webhook.resources.limits.memory": "50Mi"
        "webhook.resources.requests.cpu": "25m"
        "webhook.resources.requests.memory": "30Mi"

  PrometheusAdapter:
    Type: "AWSQS::Kubernetes::Helm"
    DependsOn: ClusterAutoscaler
    Properties:
      ClusterID: !GetAtt EKSControlPlane.Outputs.EKSName
      Name: "prometheus-adapter"
      Namespace: "kube-system"
      Chart: "prometheus-adapter"
      Repository: "https://prometheus-community.github.io/helm-charts"
      Version: "2.8.0"
      Values:
        "hostNetwork.enabled": "true"
        "dnsPolicy": "ClusterFirstWithHostNet"
        "prometheus.url": "http://sfcn-prometheus.kube-system.svc"
        "nodeSelector.sfcn\\.cisco\\.com/nodetype": "configuration"
        "serviceAccount.name": "prometheus-adapter"

  UpdatePodSecurityPolicy:
    Type: "Custom::Kubectl"
    DependsOn: ClusterAutoscaler
    Properties:
      ServiceToken: !GetAtt KubectlFunction.Arn
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      Command: |-
        patch psp/eks.privileged --patch
        '{"spec": {"allowedHostPaths": [{"pathPrefix": "/lib/modules"}, {"pathPrefix": "/local/configvol"},
        {"pathPrefix": "/tmp"}, {"pathPrefix": "/dev"}, {"pathPrefix": "/mnt/upload_to_ss"},
        {"pathPrefix": "/etc/ssl/certs", "readOnly": true}, {"pathPrefix": "/mnt/storage"},
        {"pathPrefix": "/var/log"}, {"pathPrefix": "/var/lib/docker/containers", "readOnly": true},
        {"pathPrefix": "/var/run/calico"}, {"pathPrefix": "/var/lib/calico"},
        {"pathPrefix": "/run"}, {"pathPrefix": "/sys/fs"},
        {"pathPrefix": "/opt/cni/bin"}, {"pathPrefix": "/etc/cni/net.d"},
        {"pathPrefix": "/var/lib/cni/networks"}, {"pathPrefix": "/var/run/nodeagent"},
        {"pathPrefix": "/usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds"},
        {"pathPrefix": "/mnt/coredump_repo"}, {"pathPrefix": "/home/ubuntu/fluentdout"},
        {"pathPrefix": "/var/lib/kubelet"},
        {"pathPrefix": "/var/run/efs"},
        {"pathPrefix": "/var/amazon/efs"},
        {"pathPrefix": "/etc/amazon/efs"}
        ]}}'

  EnforcerElasticache:
    Type: AWS::CloudFormation::Stack
    Condition: EnforcerCacheTypeIsElastiCache
    Properties:
      TemplateURL:
        !Sub
        - 'https://${S3Bucket}.s3.${S3Region}.${AWS::URLSuffix}/${QSS3KeyPrefix}templates/sfcn-elasticache.template.yaml'
        - S3Region: !If [ UsingDefaultBucket, !Ref 'AWS::Region', !Ref QSS3BucketRegion ]
          S3Bucket: !If [ UsingDefaultBucket, !Sub '${QSS3BucketName}-${AWS::Region}', !Ref QSS3BucketName ]
      Parameters:
        Name: !GetAtt EKSControlPlane.Outputs.EKSName
        VPCID: !Ref VPCID
        PublicSubnet1ID: !Ref PublicSubnet1ID
        PublicSubnet2ID: !Ref PublicSubnet2ID
        WorkersSecurityGroup0ID: !Ref WorkersSecurityGroup0
        WorkersSecurityGroup1ID: !Ref WorkersSecurityGroup1
        WorkersSecurityGroup2ID: !Ref WorkersSecurityGroup2
        WorkersSecurityGroup3ID: !Ref WorkersSecurityGroup3
        EnforcerCacheNodeType: !Ref EnforcerCacheNodeType
        EnforcerCacheAuthToken: !Ref EnforcerCacheAuthToken
        EnforcerCachePort: !Ref EnforcerCachePort
        EnforcerCacheTransitEncryption: !Ref EnforcerCacheTransitEncryption

  SecureFirewall:
    Type: AWS::CloudFormation::Stack
    DependsOn:
      - ClusterAutoscaler
      - CertManager
      - PrometheusAdapter
      - CleanupLambdas
    Properties:
      TemplateURL:
        !Sub
        - 'https://${S3Bucket}.s3.${S3Region}.${AWS::URLSuffix}/${QSS3KeyPrefix}templates/sfcn-helm.template.yaml'
        - S3Region: !If [ UsingDefaultBucket, !Ref 'AWS::Region', !Ref QSS3BucketRegion ]
          S3Bucket: !If [ UsingDefaultBucket, !Sub '${QSS3BucketName}-${AWS::Region}', !Ref QSS3BucketName ]
      Parameters:
        ClusterID: !GetAtt EKSControlPlane.Outputs.EKSName
        Version: !Ref FirewallVersion
        Name: "sfcn"
        SystemNamespace: !Ref SystemNamespace
        StorageType: !Ref StorageType
        Dataplane: !Ref SecureFirewallDataplane
        SmartLicenseToken: !Ref SmartLicenseToken
        MaxLicenseCount: !Ref MaxLicenseCount
        EnforcerServiceRoles: !Ref EnforcerServiceRoles
        EnforcerCacheHost: !If [ EnforcerCacheTypeIsElastiCache, !GetAtt EnforcerElasticache.Outputs.PrimaryEndpoint, "" ]
        EnforcerCachePort: !Ref EnforcerCachePort
        EnforcerCacheAuthToken: !Ref EnforcerCacheAuthToken
        EnforcerCacheStorageKey: !Ref EnforcerCacheStorageKey
        EnforcerAutoscaling: !Ref EnforcerAutoscaling
        EnforcerConfigurationTier: !Ref EnforcerConfigurationTier

  SecureFirewallCDOToken:
    Type: "Custom::Kubectl"
    DependsOn: SecureFirewall
    Properties:
      ServiceToken: !GetAtt KubectlFunction.Arn
      ClusterName: !GetAtt EKSControlPlane.Outputs.EKSName
      Decode: base64
      Command: !Sub |-
        get secrets -n ${SystemNamespace} -o=jsonpath='{.items[?(@.metadata.annotations.kubernetes\.io/service-account\.name=="sfcn-cdo")].data.token}'


Outputs:
  CAData:
    Value: !GetAtt EKSControlPlane.Outputs.CAData
  ClusterEndpoint:
    Value: !GetAtt EKSControlPlane.Outputs.EKSEndpoint
  ClusterName:
    Value: !GetAtt EKSControlPlane.Outputs.EKSName
  FirewallVersion:
    Value: !Ref FirewallVersion
    Export: { Name: !Sub '${EKSClusterName}-FirewallVersion' }
  SystemNamespace:
    Value: !Ref SystemNamespace
    Export: { Name: !Sub '${EKSClusterName}-SystemNamespace' }
  CDOToken:
    Value: !GetAtt SecureFirewallCDOToken.Output
  ElasticachePrimaryEndpoint:
    Condition: EnforcerCacheTypeIsElastiCache
    Value: !GetAtt EnforcerElasticache.Outputs.PrimaryEndpoint
    Description: CNFW Elasticache Primary Endpoint
  ElasticachePrimaryEndpointPort:
    Condition: EnforcerCacheTypeIsElastiCache
    Value: !GetAtt EnforcerElasticache.Outputs.PrimaryEndpointPort
  ElasticacheReaderEndpoint:
    Condition: EnforcerCacheTypeIsElastiCache
    Value: !GetAtt EnforcerElasticache.Outputs.ReaderEndpoint
    Description: CNFW Elasticache Reader Endpoint
  ElasticacheReaderEndpointPort:
    Condition: EnforcerCacheTypeIsElastiCache
    Value: !GetAtt EnforcerElasticache.Outputs.ReaderEndpointPort
  PublicSubnet1ID:
    Value: !Ref PublicSubnet1ID
    Export: { Name: !Sub '${EKSClusterName}-PublicSubnet1ID' }
  PublicSubnet2ID:
    Value: !Ref PublicSubnet2ID
    Export: { Name: !Sub '${EKSClusterName}-PublicSubnet2ID' }
  WorkersSecurityGroup0ID:
    Value: !Ref WorkersSecurityGroup0
    Export: { Name: !Sub '${EKSClusterName}-WorkersSecurityGroup0ID' }
  WorkersSecurityGroup1ID:
    Value: !Ref WorkersSecurityGroup1
    Export: { Name: !Sub '${EKSClusterName}-WorkersSecurityGroup1ID' }
  WorkersSecurityGroup2ID:
    Value: !Ref WorkersSecurityGroup2
    Export: { Name: !Sub '${EKSClusterName}-WorkersSecurityGroup2ID' }
  WorkersSecurityGroup3ID:
    Value: !Ref WorkersSecurityGroup3
    Export: { Name: !Sub '${EKSClusterName}-WorkersSecurityGroup3ID' }
